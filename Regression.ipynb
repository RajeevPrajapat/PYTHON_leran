{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaHEydkIWM1W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **1. What is Simple Linear Regression?**\n",
        "\n",
        "* Simple Linear Regression is a method used to examine the linear relationship between two variables: one independent (X) and one dependent (Y).\n",
        "* The goal is to predict the value of Y based on X using the equation Y = mX + c.\n",
        "* It assumes a constant rate of change and a straight-line relationship.\n",
        "* It's often used in forecasting and trend analysis.\n",
        "* The model minimizes errors using the least squares method.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "* **Linearity**: The relationship between X and Y must be linear.\n",
        "* **Independence**: Observations should be independent of each other.\n",
        "* **Homoscedasticity**: Constant variance of residuals across values of X.\n",
        "* **Normality**: Residuals should be normally distributed.\n",
        "* **No outliers**: Extreme values can distort results.\n",
        "* Violation of these assumptions can lead to biased or misleading results.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. What does the coefficient m represent in the equation Y = mX + c?**\n",
        "\n",
        "* The slope **m** represents the change in Y for every one-unit change in X.\n",
        "* If m is positive, Y increases with X; if negative, Y decreases with X.\n",
        "* It determines the steepness and direction of the regression line.\n",
        "* It quantifies the strength and direction of the relationship.\n",
        "* A larger magnitude indicates a stronger influence of X on Y.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. What does the intercept c represent in the equation Y = mX + c?**\n",
        "\n",
        "* The intercept **c** is the predicted value of Y when X is zero.\n",
        "* It marks the point where the regression line crosses the Y-axis.\n",
        "* Provides a baseline estimate of Y in the absence of X.\n",
        "* It helps understand the model's output in theoretical scenarios.\n",
        "* While not always practically interpretable, it is essential for the equation’s accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. How do we calculate the slope m in Simple Linear Regression?**\n",
        "\n",
        "* Formula:\n",
        "\n",
        "  $$\n",
        "  m = \\frac{n\\sum XY - (\\sum X)(\\sum Y)}{n\\sum X^2 - (\\sum X)^2}\n",
        "  $$\n",
        "* It minimizes the sum of squared errors between actual and predicted Y.\n",
        "* Based on covariance of X and Y divided by variance of X.\n",
        "* It ensures the best-fitting straight line.\n",
        "* Essential in building an accurate predictive model.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "\n",
        "* Minimizes the sum of squared differences between observed and predicted values.\n",
        "* Ensures the best-fit line through the data points.\n",
        "* Reduces error in prediction.\n",
        "* Provides consistent, unbiased estimates of model parameters.\n",
        "* Fundamental to regression analysis and model accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "\n",
        "* Indicates the proportion of variance in Y explained by X.\n",
        "* R² ranges from 0 to 1:\n",
        "\n",
        "  * **0**: No variance explained\n",
        "  * **1**: All variance explained\n",
        "* A higher R² means better model fit.\n",
        "* R² = 0.75 means 75% of the variation in Y is due to X.\n",
        "* Doesn’t imply causation, only correlation.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. What is Multiple Linear Regression?**\n",
        "\n",
        "* An extension of simple regression using two or more independent variables.\n",
        "* General form:\n",
        "\n",
        "  $$\n",
        "  Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_nX_n + \\epsilon\n",
        "  $$\n",
        "* Helps analyze how multiple factors affect the outcome.\n",
        "* Useful in real-world scenarios where many variables influence the result.\n",
        "* More flexible but requires careful assumptions handling.\n",
        "\n",
        "---\n",
        "\n",
        "### **9. What is the main difference between Simple and Multiple Linear Regression?**\n",
        "\n",
        "* Simple: One independent variable.\n",
        "* Multiple: Two or more independent variables.\n",
        "* Multiple regression can model more complex relationships.\n",
        "* Simple regression is easier to visualize and interpret.\n",
        "* Multiple regression requires more data and computation.\n",
        "\n",
        "---\n",
        "\n",
        "### **10. What are the key assumptions of Multiple Linear Regression?**\n",
        "\n",
        "* **Linearity** between predictors and the outcome.\n",
        "* **Independence** of errors (no autocorrelation).\n",
        "* **Homoscedasticity** of residuals.\n",
        "* **Normal distribution** of residuals.\n",
        "* **No multicollinearity** among predictors.\n",
        "* Violations affect model reliability and interpretation.\n",
        "\n",
        "---\n",
        "\n",
        "### **11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "\n",
        "* Occurs when residual variance changes with levels of an independent variable.\n",
        "* Leads to inefficient and biased estimates.\n",
        "* Affects confidence intervals and hypothesis testing.\n",
        "* Detected using residual plots or statistical tests (e.g., Breusch–Pagan).\n",
        "* Addressed using transformations, robust standard errors, or different models.\n",
        "\n",
        "---\n",
        "\n",
        "### **12. How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "\n",
        "* Remove or combine highly correlated variables.\n",
        "* Use dimensionality reduction (like PCA).\n",
        "* Apply Ridge or Lasso regression (regularization).\n",
        "* Evaluate Variance Inflation Factor (VIF) to identify collinearity.\n",
        "* Standardize variables to reduce correlations.\n",
        "\n",
        "---\n",
        "\n",
        "### **13. What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "\n",
        "* **One-hot encoding**: Converts categories into binary columns.\\n- **Label encoding**: Assigns integer values to categories.\\n- **Binary encoding**: Represents categories in binary code.\\n- **Ordinal encoding**: Maintains order in ranked categories.\\n- Categorical transformations are essential because regression requires numerical inputs.\n",
        "\n",
        "---\n",
        "\n",
        "### **14. What is the role of interaction terms in Multiple Linear Regression?**\n",
        "\n",
        "* Show how two or more variables jointly affect the outcome.\\n- Example: Effect of X₁ depends on level of X₂.\\n- Captures combined influences that individual terms miss.\\n- Important in modeling real-world complexities.\\n- Must be interpreted carefully to avoid confusion.\n",
        "\n",
        "---\n",
        "\n",
        "### **15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "\n",
        "* **Simple regression**: Intercept is Y when X = 0.\\n- **Multiple regression**: Intercept is Y when all X₁, X₂...Xₙ = 0.\\n- May not have practical meaning if zero is unrealistic.\\n- More theoretical in MLR but essential for model calculation.\n",
        "\n",
        "---\n",
        "\n",
        "### **16. What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "\n",
        "* The slope measures the rate of change in Y with respect to X.\\n- Indicates the direction (positive or negative) and strength of the relationship.\\n- Steeper slopes mean larger changes in Y.\\n- Crucial for understanding predictor importance.\\n- Helps in forecasting and decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "**17. How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "\n",
        "* The intercept indicates the expected value of the dependent variable when all independent variables are zero.\n",
        "* It serves as a baseline or starting point in the regression equation.\n",
        "* In some cases, it may have no practical meaning (e.g., if zero is not a possible value for predictors), but it's essential for accurately modeling the relationship.\n",
        "* The context of the intercept helps assess how much of the response is explained without any influence from the predictors.\n",
        "\n",
        "---\n",
        "\n",
        "**18. What are the limitations of using R² as a sole measure of model performance?**\n",
        "\n",
        "* R² does not indicate whether the regression model is appropriate or whether the predictions are unbiased.\n",
        "* It can be artificially high for overfitted models or models with many variables.\n",
        "* It does not indicate causality, only the strength of association.\n",
        "* A high R² does not guarantee good predictive performance on unseen data.\n",
        "* It does not account for the number of predictors, which is why adjusted R² is often preferred.\n",
        "\n",
        "---\n",
        "\n",
        "**19. How would you interpret a large standard error for a regression coefficient?**\n",
        "\n",
        "* A large standard error indicates high variability in the estimate of the coefficient.\n",
        "* It suggests that the corresponding predictor might not be a significant contributor to the model.\n",
        "* Large standard errors lead to wider confidence intervals, making statistical conclusions less reliable.\n",
        "* It may signal multicollinearity or poor data quality.\n",
        "\n",
        "---\n",
        "\n",
        "**20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "\n",
        "* In a residual plot, heteroscedasticity appears as a funnel shape—residuals spread wider or narrower as the fitted values increase.\n",
        "* It violates one of the key assumptions of linear regression.\n",
        "* It can lead to inefficient estimates, biased standard errors, and unreliable hypothesis tests.\n",
        "* Techniques to address it include data transformation, weighted least squares, or using robust standard errors.\n",
        "\n",
        "**21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        "\n",
        "* It suggests that adding more predictors has increased the R² but not improved model quality.\n",
        "* Adjusted R² accounts for the number of predictors and only increases if the new variables improve the model significantly.\n",
        "* A high R² but low adjusted R² may indicate overfitting.\n",
        "* It calls for reevaluation of predictor variables and model simplification.\n",
        "\n",
        "**22. Why is it important to scale variables in Multiple Linear Regression?**\n",
        "\n",
        "* Scaling ensures that all variables contribute equally to the model.\n",
        "* It improves numerical stability and convergence speed, especially in models using regularization (e.g., Ridge, Lasso).\n",
        "* It makes coefficients more interpretable.\n",
        "* Prevents predictors with large scales from dominating those with smaller scales.\n",
        "\n",
        "**23. What is polynomial regression?**\n",
        "\n",
        "* Polynomial regression is a type of regression analysis where the relationship between the independent and dependent variable is modeled as an nth-degree polynomial.\n",
        "* It allows fitting curves to the data rather than straight lines.\n",
        "* For example, Y = β₀ + β₁X + β₂X² + ... + βₙXⁿ + ε\n",
        "* It is used when data shows a curvilinear trend.\n",
        "\n",
        "**24. How does polynomial regression differ from linear regression?**\n",
        "\n",
        "* Linear regression models linear relationships, while polynomial regression can model nonlinear patterns.\n",
        "* Polynomial regression includes higher-order powers of the independent variable.\n",
        "* Linear regression fits a straight line, polynomial fits curves.\n",
        "* Both are linear in coefficients but differ in the functional form.\n",
        "\n",
        "**25. When is polynomial regression used?**\n",
        "\n",
        "* When the relationship between the dependent and independent variables is nonlinear.\n",
        "* When residual plots indicate patterns that a linear model cannot capture.\n",
        "* For modeling growth rates, wave patterns, and U-shaped relationships.\n",
        "* To improve prediction accuracy when linear models underperform.\n",
        "\n",
        "**26. What is the general equation for polynomial regression?**\n",
        "\n",
        "* Y = β₀ + β₁X + β₂X² + β₃X³ + ... + βₙXⁿ + ε\n",
        "* Where β are coefficients, X is the independent variable, and n is the degree of the polynomial.\n",
        "* It captures more complex relationships as the degree increases.\n",
        "\n",
        "**27. Can polynomial regression be applied to multiple variables?**\n",
        "\n",
        "* Yes, it can be extended to multiple variables (multivariate polynomial regression).\n",
        "* Includes interaction terms and polynomial terms for each predictor.\n",
        "* For example: Y = β₀ + β₁X₁ + β₂X₁² + β₃X₂ + β₄X₂² + β₅X₁X₂ + ε\n",
        "* Complexity increases significantly with more variables.\n",
        "\n",
        "**28. What are the limitations of polynomial regression?**\n",
        "\n",
        "* Prone to overfitting, especially with high-degree polynomials.\n",
        "* Poor extrapolation beyond the range of the data.\n",
        "* Interpretation becomes difficult as the degree increases.\n",
        "* Higher degrees can lead to large variance and numerical instability.\n",
        "\n",
        "**29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "\n",
        "* Use cross-validation techniques (like k-fold CV).\n",
        "* Compare RMSE or MAE for different degrees.\n",
        "* Analyze adjusted R² and AIC/BIC values.\n",
        "* Use validation curves to visualize performance against complexity.\n",
        "\n",
        "**30. Why is visualization important in polynomial regression?**\n",
        "\n",
        "* Helps assess how well the model captures the data's shape.\n",
        "* Makes it easier to detect overfitting or underfitting.\n",
        "* Visual residual plots can indicate if the model is appropriate.\n",
        "* Useful for communicating findings and interpreting relationships.\n",
        "\n",
        "**31. How is polynomial regression implemented in Python?**\n",
        "\n",
        "* Use `PolynomialFeatures` from `sklearn.preprocessing` to generate polynomial terms.\n",
        "* Combine with `LinearRegression` from `sklearn.linear_model`:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "* `make_pipeline` ensures clean and sequential preprocessing.\n",
        "* Evaluate using R² score, residuals, and visualization.\n",
        "\n",
        "**End of Answer Sheet**\n"
      ],
      "metadata": {
        "id": "ZsGtKNcfXJek"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qI7STisuX8Ay"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}