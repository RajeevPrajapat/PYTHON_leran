{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DgMPBGtex-Hx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "// 1. What is a parameter?\n",
        "In machine learning, a parameter is a variable that the learning algorithm automatically learns from the training data. These parameters define the model’s behavior. For example, in a linear regression model, the coefficients (weights) and intercept are the parameters learned during training to make accurate predictions.\n",
        "\n",
        "// 2. What is correlation?\n",
        "Correlation is a statistical measure that indicates the extent to which two or more variables fluctuate together. A high correlation means that when one variable changes, the other tends to change in a predictable way. It can be positive (both increase together), negative (one increases while the other decreases), or zero (no relationship).\n",
        "\n",
        "// 3. What does negative correlation mean?\n",
        "Negative correlation means that as the value of one variable increases, the value of the other variable tends to decrease. For example, as the speed of a car increases, the time taken to reach a destination decreases. A correlation coefficient close to -1 indicates a strong negative correlation.\n",
        "\n",
        "// 4. Define Machine Learning. What are the main components in Machine Learning?\n",
        "Machine Learning (ML) is a branch of artificial intelligence that enables systems to learn patterns from data and improve their performance without being explicitly programmed. The main components of ML include:\n",
        "- Data: The input used for learning.\n",
        "- Model: The mathematical representation of a system.\n",
        "- Algorithm: The method used to train the model.\n",
        "- Loss Function: Measures the error between predicted and actual values.\n",
        "- Optimizer: Minimizes the loss function and improves the model's performance.\n",
        "\n",
        "// 5. How does loss value help in determining whether the model is good or not?\n",
        "The loss value quantifies the difference between the predicted values and actual values. A lower loss value indicates a better-performing model, while a high loss value suggests the model is making large prediction errors. It is a key indicator used during training to evaluate and optimize the model.\n",
        "\n",
        "// 6. What are continuous and categorical variables?\n",
        "Continuous variables are numerical values that can take any number within a range, such as height, weight, or temperature. Categorical variables represent groups or categories, such as gender, color, or product type. These must often be encoded into numbers for machine learning models.\n",
        "\n",
        "// 7. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "Categorical variables need to be converted into numerical format before being used in machine learning algorithms. Common techniques include:\n",
        "- Label Encoding: Assigns each category a unique integer.\n",
        "- One-Hot Encoding: Creates binary columns for each category.\n",
        "- Ordinal Encoding: Used for categorical data with a meaningful order.\n",
        "\n",
        "// 8. What do you mean by training and testing a dataset?\n",
        "Training a dataset means using a portion of the data to teach the model and help it learn patterns. Testing a dataset means using another portion of data to evaluate how well the model has learned. The model should perform well on both the training and testing datasets to ensure it can generalize to new data.\n",
        "\n",
        "// 9. What is sklearn.preprocessing?\n",
        "`sklearn.preprocessing` is a module in the scikit-learn library that provides functions to prepare data for machine learning models. It includes tools for scaling, encoding categorical variables, normalization, binarization, and more. These steps ensure the model gets clean, consistent input.\n",
        "\n",
        "// 10. What is a Test set?\n",
        "A test set is the subset of the dataset used to evaluate the model after it has been trained. It acts like new, unseen data and helps in checking the model's performance and generalization ability. The test set is not involved during training to ensure unbiased evaluation.\n",
        "\n",
        "// 11. How do we split data for model fitting (training and testing) in Python?\n",
        "In Python, we can use `train_test_split()` from the `sklearn.model_selection` module to split the data. It randomly divides the data into training and testing sets. For example:\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "// 12. How do you approach a Machine Learning problem?\n",
        "A structured approach includes the following steps:\n",
        "\n",
        "Define the problem clearly.\n",
        "\n",
        "Collect and understand the data.\n",
        "\n",
        "Clean and preprocess the data (handling null values, encoding, scaling).\n",
        "\n",
        "Perform Exploratory Data Analysis (EDA).\n",
        "\n",
        "Select appropriate features.\n",
        "\n",
        "Choose a suitable algorithm.\n",
        "\n",
        "Train the model.\n",
        "\n",
        "Evaluate the model using metrics.\n",
        "\n",
        "Tune hyperparameters if needed.\n",
        "\n",
        "Deploy the model.\n",
        "\n",
        "// 13. Why do we have to perform EDA before fitting a model to the data?\n",
        "EDA (Exploratory Data Analysis) helps us understand the dataset better by summarizing its main characteristics using visual and statistical techniques. It allows us to identify patterns, detect outliers, and find correlations. This step is crucial before building models, as it guides data cleaning, feature selection, and model choice.\n",
        "\n",
        "// 14. What is correlation?\n",
        "Correlation is a statistical method used to measure the strength and direction of a relationship between two variables. (Same as Q2)\n",
        "\n",
        "// 15. What does negative correlation mean?\n",
        "Negative correlation indicates an inverse relationship between two variables. (Same as Q3)\n",
        "\n",
        "// 16. How can you find correlation between variables in Python?\n",
        "We can use the corr() function provided by the pandas library:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import pandas as pd\n",
        "df = pd.read_csv('data.csv')\n",
        "correlation_matrix = df.corr()\n",
        "print(correlation_matrix)\n",
        "This will output the correlation coefficients between all numerical features.\n",
        "\n",
        "// 17. What is causation? Explain difference between correlation and causation with an example.\n",
        "Causation means that one variable directly affects the other. Correlation just shows that two variables move together but doesn't prove one causes the other.\n",
        "Example: Ice cream sales and drowning incidents may both increase in summer (correlation), but eating ice cream doesn’t cause drowning (no causation). The real cause is hot weather.\n",
        "\n",
        "// 18. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "An optimizer is an algorithm that adjusts the model’s weights to reduce the loss during training.\n",
        "Types include:\n",
        "\n",
        "SGD (Stochastic Gradient Descent): Updates weights for each data point, slower but simple.\n",
        "\n",
        "Adam: Combines momentum and adaptive learning rate; very popular.\n",
        "\n",
        "RMSProp: Maintains a moving average of squared gradients; good for non-stationary data.\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(optimizer=Adam(), loss='mse')\n",
        "// 19. What is sklearn.linear_model?\n",
        "sklearn.linear_model is a module in scikit-learn that contains linear models for regression and classification tasks. Examples include LinearRegression, LogisticRegression, and Ridge. These models assume a linear relationship between input features and target variable.\n",
        "\n",
        "// 20. What does model.fit() do? What arguments must be given?\n",
        "The fit() method trains the machine learning model on the given data.\n",
        "Arguments:\n",
        "\n",
        "X_train: Feature data.\n",
        "\n",
        "y_train: Target labels.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "model.fit(X_train, y_train)\n",
        "// 21. What does model.predict() do? What arguments must be given?\n",
        "The predict() method uses the trained model to make predictions on new data.\n",
        "Argument:\n",
        "\n",
        "X_test: The data to make predictions on.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "y_pred = model.predict(X_test)\n",
        "// 22. What are continuous and categorical variables?\n",
        "(Already covered in Q6)\n",
        "\n",
        "// 23. What is feature scaling? How does it help in Machine Learning?\n",
        "Feature scaling is the process of normalizing or standardizing the range of independent variables. It is important because models like SVM and KNN are sensitive to feature magnitude. Scaling helps speed up training and leads to better performance.\n",
        "\n",
        "// 24. How do we perform scaling in Python?\n",
        "We use classes like StandardScaler or MinMaxScaler from sklearn.preprocessing.\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "// 25. What is sklearn.preprocessing?\n",
        "(Already covered in Q9)\n",
        "\n",
        "// 26. How do we split data for model fitting (training and testing) in Python?\n",
        "(Already covered in Q11)\n",
        "\n",
        "// 27. Explain data encoding?\n",
        "Data encoding is the process of converting categorical data into numerical form so that it can be used by machine learning models. Techniques include:\n",
        "\n",
        "Label Encoding: Assigns each label a unique number.\n",
        "\n",
        "One-Hot Encoding: Converts each category into a binary column (0 or 1).\n",
        "These techniques ensure that models can process non-numeric data properly.\n",
        "\n",
        "yaml\n",
        "Copy\n",
        "Edit\n",
        "\n",
        "---\n",
        "\n",
        "Let me know if you'd like this exported to a **PDF or Word document**, or if you want explanations"
      ],
      "metadata": {
        "id": "vgd-QVVhyBk3"
      }
    }
  ]
}